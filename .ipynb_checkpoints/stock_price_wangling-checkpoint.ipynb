{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, year, month, sum as spark_sum, avg as spark_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Libs\n",
    "def download_yfinance_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date).reset_index()\n",
    "    data.columns = data.columns.droplevel(-1)\n",
    "    data['Stock'] = ticker\n",
    "    data['Source'] = 'yfinance'\n",
    "    return data\n",
    "\n",
    "def download_alpha_vantage_data(ticker, api_key):\n",
    "    #url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&apikey={api_key}&outputsize=full\"\n",
    "    url = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=IBM&outputsize=full&apikey=demo\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    data = pd.DataFrame.from_dict(response.json()['Time Series (Daily)'], orient= 'index').sort_index(axis=1)\n",
    "    data.rename(columns={\n",
    "        'timestamp': 'Date',\n",
    "        '1. open': 'Open',\n",
    "        '2. high': 'High',\n",
    "        '3. low': 'Low',\n",
    "        '4. close': 'Close',\n",
    "        '5. adjusted close': 'Adj Close',\n",
    "        '6. volume': 'Volume'\n",
    "    }, inplace=True)\n",
    "    data['Stock'] = \"NVDA\"\n",
    "    data['Source'] = 'alpha vantage'\n",
    "    data['Date'] = data.index\n",
    "    data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Stock', 'Source']].reset_index(drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from secret import credential\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2024-01-01\"\n",
    "tickers = ['IBM']\n",
    "alpha_vantage_api_key = credential.ALPHA_VANTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Download stock data from yfinance and alpha_vantage\n",
    "yfinance_data = pd.concat([download_yfinance_data(ticker, start_date, end_date) for ticker in tickers])\n",
    "alpha_vantage_data = pd.concat([download_alpha_vantage_data(ticker, alpha_vantage_api_key) for ticker in tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>207.46</td>\n",
       "      <td>208.49</td>\n",
       "      <td>204.07</td>\n",
       "      <td>204.99</td>\n",
       "      <td>204.99</td>\n",
       "      <td>3986460</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-14</td>\n",
       "      <td>210.0</td>\n",
       "      <td>210.4999</td>\n",
       "      <td>206.35</td>\n",
       "      <td>208.99</td>\n",
       "      <td>208.99</td>\n",
       "      <td>6372853</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>209.5</td>\n",
       "      <td>211.41</td>\n",
       "      <td>209.0701</td>\n",
       "      <td>210.92</td>\n",
       "      <td>210.92</td>\n",
       "      <td>3247830</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-12</td>\n",
       "      <td>211.9</td>\n",
       "      <td>213.03</td>\n",
       "      <td>209.06</td>\n",
       "      <td>210.86</td>\n",
       "      <td>210.86</td>\n",
       "      <td>2818216</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-11</td>\n",
       "      <td>214.4</td>\n",
       "      <td>215.41</td>\n",
       "      <td>213.48</td>\n",
       "      <td>213.57</td>\n",
       "      <td>211.891827977227</td>\n",
       "      <td>3012987</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>1999-11-05</td>\n",
       "      <td>92.75</td>\n",
       "      <td>92.94</td>\n",
       "      <td>90.19</td>\n",
       "      <td>90.25</td>\n",
       "      <td>45.7112254831793</td>\n",
       "      <td>13737600</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>1999-11-04</td>\n",
       "      <td>94.44</td>\n",
       "      <td>94.44</td>\n",
       "      <td>90.0</td>\n",
       "      <td>91.56</td>\n",
       "      <td>46.3747346840985</td>\n",
       "      <td>16697600</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>1999-11-03</td>\n",
       "      <td>95.87</td>\n",
       "      <td>95.94</td>\n",
       "      <td>93.5</td>\n",
       "      <td>94.37</td>\n",
       "      <td>47.7979872448491</td>\n",
       "      <td>10369100</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>1999-11-02</td>\n",
       "      <td>96.75</td>\n",
       "      <td>96.81</td>\n",
       "      <td>93.69</td>\n",
       "      <td>94.81</td>\n",
       "      <td>48.0208452970662</td>\n",
       "      <td>11105400</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>1999-11-01</td>\n",
       "      <td>98.5</td>\n",
       "      <td>98.81</td>\n",
       "      <td>96.37</td>\n",
       "      <td>96.75</td>\n",
       "      <td>49.0034467091146</td>\n",
       "      <td>9551800</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>alpha vantage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6302 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open      High       Low   Close         Adj Close  \\\n",
       "0     2024-11-15  207.46    208.49    204.07  204.99            204.99   \n",
       "1     2024-11-14   210.0  210.4999    206.35  208.99            208.99   \n",
       "2     2024-11-13   209.5    211.41  209.0701  210.92            210.92   \n",
       "3     2024-11-12   211.9    213.03    209.06  210.86            210.86   \n",
       "4     2024-11-11   214.4    215.41    213.48  213.57  211.891827977227   \n",
       "...          ...     ...       ...       ...     ...               ...   \n",
       "6297  1999-11-05   92.75     92.94     90.19   90.25  45.7112254831793   \n",
       "6298  1999-11-04   94.44     94.44      90.0   91.56  46.3747346840985   \n",
       "6299  1999-11-03   95.87     95.94      93.5   94.37  47.7979872448491   \n",
       "6300  1999-11-02   96.75     96.81     93.69   94.81  48.0208452970662   \n",
       "6301  1999-11-01    98.5     98.81     96.37   96.75  49.0034467091146   \n",
       "\n",
       "        Volume Stock         Source  \n",
       "0      3986460  NVDA  alpha vantage  \n",
       "1      6372853  NVDA  alpha vantage  \n",
       "2      3247830  NVDA  alpha vantage  \n",
       "3      2818216  NVDA  alpha vantage  \n",
       "4      3012987  NVDA  alpha vantage  \n",
       "...        ...   ...            ...  \n",
       "6297  13737600  NVDA  alpha vantage  \n",
       "6298  16697600  NVDA  alpha vantage  \n",
       "6299  10369100  NVDA  alpha vantage  \n",
       "6300  11105400  NVDA  alpha vantage  \n",
       "6301   9551800  NVDA  alpha vantage  \n",
       "\n",
       "[6302 rows x 9 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_vantage_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03 00:00:00+00:00</td>\n",
       "      <td>130.147705</td>\n",
       "      <td>141.550003</td>\n",
       "      <td>141.899994</td>\n",
       "      <td>140.479996</td>\n",
       "      <td>141.100006</td>\n",
       "      <td>3338600</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04 00:00:00+00:00</td>\n",
       "      <td>131.113129</td>\n",
       "      <td>142.600006</td>\n",
       "      <td>143.619995</td>\n",
       "      <td>141.369995</td>\n",
       "      <td>142.070007</td>\n",
       "      <td>3869200</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05 00:00:00+00:00</td>\n",
       "      <td>129.743164</td>\n",
       "      <td>141.110001</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>140.009995</td>\n",
       "      <td>142.440002</td>\n",
       "      <td>2866600</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06 00:00:00+00:00</td>\n",
       "      <td>132.124512</td>\n",
       "      <td>143.699997</td>\n",
       "      <td>144.250000</td>\n",
       "      <td>141.580002</td>\n",
       "      <td>142.380005</td>\n",
       "      <td>3574000</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09 00:00:00+00:00</td>\n",
       "      <td>131.986603</td>\n",
       "      <td>143.550003</td>\n",
       "      <td>145.470001</td>\n",
       "      <td>143.399994</td>\n",
       "      <td>144.080002</td>\n",
       "      <td>3987700</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2023-12-22 00:00:00+00:00</td>\n",
       "      <td>156.483139</td>\n",
       "      <td>162.139999</td>\n",
       "      <td>162.410004</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>161.100006</td>\n",
       "      <td>2439800</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2023-12-26 00:00:00+00:00</td>\n",
       "      <td>157.515808</td>\n",
       "      <td>163.210007</td>\n",
       "      <td>163.309998</td>\n",
       "      <td>162.050003</td>\n",
       "      <td>162.229996</td>\n",
       "      <td>1772400</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2023-12-27 00:00:00+00:00</td>\n",
       "      <td>157.757095</td>\n",
       "      <td>163.460007</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>162.679993</td>\n",
       "      <td>163.139999</td>\n",
       "      <td>3234600</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2023-12-28 00:00:00+00:00</td>\n",
       "      <td>158.036957</td>\n",
       "      <td>163.750000</td>\n",
       "      <td>163.960007</td>\n",
       "      <td>163.399994</td>\n",
       "      <td>163.960007</td>\n",
       "      <td>2071300</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2023-12-29 00:00:00+00:00</td>\n",
       "      <td>157.843948</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>164.179993</td>\n",
       "      <td>162.830002</td>\n",
       "      <td>163.750000</td>\n",
       "      <td>2525600</td>\n",
       "      <td>IBM</td>\n",
       "      <td>yfinance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                      Date   Adj Close       Close        High  \\\n",
       "0     2023-01-03 00:00:00+00:00  130.147705  141.550003  141.899994   \n",
       "1     2023-01-04 00:00:00+00:00  131.113129  142.600006  143.619995   \n",
       "2     2023-01-05 00:00:00+00:00  129.743164  141.110001  142.500000   \n",
       "3     2023-01-06 00:00:00+00:00  132.124512  143.699997  144.250000   \n",
       "4     2023-01-09 00:00:00+00:00  131.986603  143.550003  145.470001   \n",
       "..                          ...         ...         ...         ...   \n",
       "245   2023-12-22 00:00:00+00:00  156.483139  162.139999  162.410004   \n",
       "246   2023-12-26 00:00:00+00:00  157.515808  163.210007  163.309998   \n",
       "247   2023-12-27 00:00:00+00:00  157.757095  163.460007  163.639999   \n",
       "248   2023-12-28 00:00:00+00:00  158.036957  163.750000  163.960007   \n",
       "249   2023-12-29 00:00:00+00:00  157.843948  163.550003  164.179993   \n",
       "\n",
       "Price         Low        Open   Volume Stock    Source  \n",
       "0      140.479996  141.100006  3338600   IBM  yfinance  \n",
       "1      141.369995  142.070007  3869200   IBM  yfinance  \n",
       "2      140.009995  142.440002  2866600   IBM  yfinance  \n",
       "3      141.580002  142.380005  3574000   IBM  yfinance  \n",
       "4      143.399994  144.080002  3987700   IBM  yfinance  \n",
       "..            ...         ...      ...   ...       ...  \n",
       "245    161.000000  161.100006  2439800   IBM  yfinance  \n",
       "246    162.050003  162.229996  1772400   IBM  yfinance  \n",
       "247    162.679993  163.139999  3234600   IBM  yfinance  \n",
       "248    163.399994  163.960007  2071300   IBM  yfinance  \n",
       "249    162.830002  163.750000  2525600   IBM  yfinance  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yfinance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to PySpark\n",
    "spark = SparkSession.builder.appName(\"Stock Data Preparation\").getOrCreate()\n",
    "\n",
    "yfinance_df = spark.createDataFrame(yfinance_data)\n",
    "alpha_vantage_df = spark.createDataFrame(alpha_vantage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- Stock: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- Stock: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yfinance_df.printSchema()\n",
    "alpha_vantage_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the data type of the two PySpark Dataframes\n",
    "yfinance_df = yfinance_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyyMMdd\"))\n",
    "alpha_vantage_df = alpha_vantage_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyyMMdd\"))\n",
    "\n",
    "double_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "for double_col in double_cols:\n",
    "    alpha_vantage_df = alpha_vantage_df.withColumn(double_col, col(double_col).cast(\"double\"))\n",
    "\n",
    "alpha_vantage_df = alpha_vantage_df.withColumn(\"Volume\", col(\"Volume\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- Stock: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "py_df = yfinance_df.union(alpha_vantage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o135.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 4.0 failed 1 times, most recent failure: Lost task 6.0 in stage 4.0 (TID 24) (DESKTOP-FA6QR84 executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpy_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Github\\apache_spark\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\Github\\apache_spark\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\Github\\apache_spark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\Github\\apache_spark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Documents\\Github\\apache_spark\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o135.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 4.0 failed 1 times, most recent failure: Lost task 6.0 in stage 4.0 (TID 24) (DESKTOP-FA6QR84 executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "py_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove na and duplicate values\n",
    "yfinance_df = yfinance_df.na.drop().dropDuplicates()\n",
    "alpha_vantage_df = alpha_vantage_df.na.drop().dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust date and create week/month/year columns\n",
    "for df in [yfinance_df, alpha_vantage_df]:\n",
    "    df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
    "    df = df.withColumn(\"Month\", month(col(\"Date\")))\n",
    "    df = df.withColumn(\"Week\", weekofyear(col(\"Date\")))\n",
    "\n",
    "# Step 8: Validate data between the two sources (simple join example)\n",
    "joined_df = yfinance_df.join(alpha_vantage_df, [\"Date\", \"Stock\"], \"inner\")\n",
    "joined_df.show(5)\n",
    "\n",
    "# Step 9: Analysis - Add average/sum by group\n",
    "result_df = yfinance_df.groupBy(\"Stock\", \"Year\", \"Month\").agg(\n",
    "    spark_avg(\"Close\").alias(\"Avg_Close\"),\n",
    "    spark_sum(\"Volume\").alias(\"Total_Volume\")\n",
    ")\n",
    "result_df.show(5)\n",
    "\n",
    "# Step 10: Spark SQL\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "result_df.createOrReplaceTempView(\"stock_analysis\")\n",
    "\n",
    "# Define and execute an SQL query\n",
    "query = \"\"\"\n",
    "SELECT Stock, Year, Month, Avg_Close, Total_Volume\n",
    "FROM stock_analysis\n",
    "WHERE Total_Volume > 1000000\n",
    "ORDER BY Year, Month, Stock\n",
    "\"\"\"\n",
    "query_result = spark.sql(query)\n",
    "query_result.show(5)\n",
    "\n",
    "# Step 11: Write the output to Parquet\n",
    "query_result.write.mode(\"overwrite\").parquet(\"output/stock_analysis_result.parquet\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wangling\n",
    "1. Study table info, head, schema\n",
    "2. Check na and remove duplicates\n",
    "3. Check abnormal values\n",
    "4. Adjust Date, generate week/month/year\n",
    "5. Convert data format\n",
    "6. validate data from 2 sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "1. add average/sum by group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "1. define schema\n",
    "2. execute query\n",
    "3. Write the output query to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "Put the process to streaming, for real-time process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
